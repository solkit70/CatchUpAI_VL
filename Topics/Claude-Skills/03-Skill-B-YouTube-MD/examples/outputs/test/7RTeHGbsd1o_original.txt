[00:03] All right. Uh good morning. Uh my name
[00:07] is Charles and uh I'm a co-founder CEO
[00:10] of MBER. It's great to uh be here with
[00:14] uh really my uh colleagues and my
[00:18] comrades in this uh venture uh into AI
[00:23] memory. Um I want to share uh my uh
[00:27] perspective maybe one perspective on
[00:30] what AI memory is and why I think it is
[00:33] important for the uh authentic AI
[00:38] revolution.
[00:40] Okay. So um you know first of all is
[00:44] something that everyone knows uh that
[00:48] large language model is becoming the new
[00:51] computer. uh before we have a problem uh
[00:55] we uh give it to the computer uh our
[00:58] laptop our PC server or a supercomput
[01:03] and it will compute and give us the
[01:06] results and that's the world we've been
[01:09] used to for the last 60 years or so and
[01:13] uh in the last few years uh we are
[01:17] adapting to another mode which is when
[01:19] we have a problem instead of having
[01:22] computer instead of writing programs and
[01:24] and letting computer answer we ask a
[01:27] model and model would give us the
[01:29] results. So from that perspective uh
[01:32] model is a new computer. It is there to
[01:35] answer questions, to solve problems for
[01:38] us. And to do that, we have agents and
[01:40] assistants
[01:42] uh that are really digital colleagues
[01:45] for us to help us uh using the models to
[01:50] give us the right answers. And while all
[01:52] these new computation are happening,
[01:57] uh there are data being generated just
[01:59] like the old computer. when the whole
[02:00] computer computes their data being
[02:03] generated and then it needs to be saved
[02:06] somewhere and those data can be further
[02:09] used for further computation later on
[02:13] and with this AI the computer they're
[02:15] also data being generated uh these are
[02:18] the data generated by model agents or by
[02:21] the users um now these data needs to be
[02:25] stored somewhere too uh if you want to
[02:28] remember them and model itself cannot
[02:32] remember by design. In fact, it's a
[02:35] problem if the model can't remember
[02:37] during the inference uh then um you lose
[02:41] all the privacy uh of your data when
[02:44] you're using model to do so. So there
[02:47] need to be a separate system that sort
[02:50] of a new database or a new storage for
[02:53] the data that the app generates. And in
[02:55] my point of view uh that system is AI
[03:00] memory. So AI memory is the new data
[03:04] platform
[03:05] uh for
[03:07] uh the AI generated data and uh um and
[03:12] and this is now um responsible for
[03:16] remembering them and recalling them at
[03:18] the right time so that the right context
[03:22] can be delivered to the model at the
[03:24] right time. and context is very
[03:27] important for the accuracy and
[03:30] personalization of the AI responses uh
[03:34] from models to the question to those
[03:36] questions and uh so the memory will need
[03:39] to do that. Now you might ask why
[03:42] couldn't we just use a classic storage
[03:44] or classic database to do so. Um and in
[03:48] our point of view and I think probably
[03:51] all of your point of view that this
[03:54] requirement for this AI memory is quite
[03:57] different from the classical databases
[04:00] or data stores. Uh just like human
[04:03] memory
[04:05] the memory is a living thing. It's
[04:07] something that's active. It's something
[04:09] that's intelligent. Uh it is not just
[04:13] something you store in the tree. Those
[04:14] are not the only not only for them get
[04:17] those are not the only few actions you
[04:19] have with your memory. Uh but rather it
[04:22] is something that's constantly evolving.
[04:25] It has a life of its own. It decays uh
[04:29] it consolidates
[04:31] uh and it reorganizes itself uh through
[04:35] other relations and connections. So it
[04:39] is a living thing by itself. it is
[04:42] active uh and and that's something that
[04:46] the storage of database are not and
[04:49] those are just put and get are the only
[04:52] thing you do like you you add memory or
[04:54] search and but memory itself in the AI
[04:58] world is actually an agent in a cell it
[05:01] is something that is uh autonomous
[05:04] that's something that is uh is proactive
[05:07] and over time this memory machine in a
[05:12] way uh that can proactively learns and
[05:16] adapts and really retains the
[05:20] institutional knowledge that whether it
[05:22] is for an enterprise whether it is for
[05:25] other organizations or whether it's for
[05:27] individuals that forms the essence of
[05:31] what that enterprise knows and I think
[05:34] over time AI generated data will become
[05:37] the majority of all data And this is
[05:41] different type of data and they will be
[05:42] stored in AI memory and therefore make
[05:45] AI memory the most valuable asset to any
[05:49] institutions or enterprises or
[05:52] individuals. You know our memory is what
[05:55] make Charles Charles and I believe the
[05:59] AI memory for enterprise A is what's
[06:03] going to make the enterprise A the
[06:06] Enterprise A. So, so this is over time
[06:08] going to be the most valuable uh essence
[06:11] of an organization.
[06:13] Models and agents
[06:16] come and go. Um and uh you nowadays when
[06:20] we talk to enterprise customers there
[06:22] are hundreds or thousands of agents
[06:25] being driven everywhere. Every group
[06:28] within enterprises try to reposition
[06:30] themselves as a agent team and there are
[06:34] hundreds of agent team in a large
[06:35] enterprise and they are all developing
[06:38] agents of themselves and these agents
[06:40] will evolve. uh there's really a sprawl
[06:43] of all these agents that's emerging and
[06:48] AI memory will also serve as a common
[06:51] data fabric that all these agents all
[06:54] these models behind these agents can
[06:57] share so that you can you know putting a
[07:01] common fabric that can connect to this
[07:04] sprawl of agents that are happening
[07:07] today and continue happening tomorrow.
[07:10] Okay, so that's you know a particular
[07:12] angle into AI memory. Um there are
[07:16] certainly other angles into it. It it
[07:19] can make uh the models uh perform
[07:22] better. It can make the agents easier to
[07:25] develop. It has many other benefits but
[07:28] this is one perspective that we believe
[07:30] AI memory will form the data
[07:33] underpinning of the AI centric world
[07:37] that are coming
[07:39] and and that makes this event
[07:41] significant. I think we are the early
[07:43] explorers in this world and uh and
[07:47] that's also the reason we would like to
[07:48] invite all the folks that we can find
[07:51] who are also working in this field uh
[07:53] because I think this is going to be a
[07:55] huge field and it's going to take the
[07:56] effort of all of us to make this happen
[08:00] sooner rather than later and we can
[08:02] collaborate together in this uh not only
[08:06] in information sharing but potentially
[08:09] in in the standards creation or
[08:11] benchmark uh creation so that we can
[08:14] together advancing this uh field
[08:16] forward.
[08:18] Okay. Now the the industry is also you
[08:22] know their announcement happening
[08:23] everywhere uh from the big companies to
[08:27] many of the startups like uh us um the
[08:32] open AI you know with GPT5 and GPT6 that
[08:37] they are forecasting are putting a lot
[08:39] of emphasis on the memory they are
[08:41] creating. Uh cloud is uh catching up and
[08:45] uh announcing various um memory
[08:49] features. Google has a big announcement
[08:51] this week and Gemini is having various
[08:54] memory features uh introducing as well.
[08:58] Now these are all separate system from
[09:01] the model from the flagship model they
[09:03] have uh but they are integrate part of
[09:06] their service and it's tightly coupled
[09:09] uh to their model and uh you know I
[09:12] think probably not for technical reason
[09:14] but for other reasons their memory
[09:17] systems would only support their own
[09:20] models. This will make the user
[09:22] experience better with their models and
[09:24] this will also make the user more
[09:26] sticky. uh to their models. Uh you know
[09:30] some of you and and myself probably have
[09:32] the experience once you have the memory
[09:34] built the experience the connection you
[09:37] have with models will be much closer
[09:39] than without the memory and but that I
[09:42] think does leave room for the other
[09:44] players because I think this memory
[09:47] needs to be converged. I don't want to
[09:50] have a 100 different piece of memories
[09:52] if I'm using a 100 different AI
[09:54] services. I I just want to have one. I
[09:56] want wanted it to know me. I wanted to
[10:00] capture data from all the AI service I
[10:03] use so they can benefit from each other
[10:05] and and I want my memory to belong to me
[10:08] not belong to one of those services and
[10:11] uh and I think that's something we could
[10:13] build um you know whether it's an open
[10:15] source or as a product or service that's
[10:19] really giving the owner of the memory
[10:21] the control to the memory they have
[10:24] rather than relinquish that control to
[10:26] the particular models And um and then
[10:30] there are you know other uh news from
[10:32] various sources uh that and people are
[10:36] predicting uh a huge huge market here um
[10:40] that is being uh created and uh it is
[10:44] because of this um in memory
[10:52] centric infrastructure. uh we were at
[10:55] lower level trying to build an
[10:56] infrastructure
[10:58] uh leveraging some emerging new hardware
[11:01] um and uh over the last couple years we
[11:05] have shifted uh pivoted our attention to
[11:09] this field of AI memory and uh we've
[11:12] been making on a uh project uh open
[11:16] source project called project man and uh
[11:20] this is our effort uh in uh attacking
[11:24] the AI memory problem and uh and it is
[11:29] uh available on GitHub. Uh we open
[11:33] sourced it a few weeks ago and uh and
[11:36] here is a simplistic uh block background
[11:41] uh describing what's included in that
[11:45] project. Um and this blue box is our
[11:49] memory system and it is accessible by
[11:52] various models, agents, assistance or
[11:56] directly by users through a various uh
[11:58] APIs uh or interfaces. Um it has a MCP
[12:03] server that can be visited directly by
[12:06] models or by agents or by other uh
[12:10] software. uh it has a HTTP and HTTPS
[12:15] endpoint. Uh that's the restful API that
[12:18] can also be called by any other piece of
[12:21] software. It's also uh an Python SDK
[12:26] that can be integrated with your agents
[12:29] and your applications directly. And
[12:32] within the memory uh system that uh we
[12:38] have uh three piece of memory two of
[12:42] these are available today. The other
[12:44] piece we are still working out and we
[12:46] haven't open source yet. Uh the first
[12:48] piece is episodic memory and this is
[12:51] where you remember the events the
[12:53] episodes and how do you
[12:56] uh uh organize them into memory entities
[12:59] and creating the right relations between
[13:01] the entities so that uh the right
[13:04] episodes or right memory entities can be
[13:08] called to deliver the context for all
[13:11] future queries. And within episodic we
[13:14] have two modules. One is the shortterm
[13:17] working memory and this is all stored
[13:20] into DRAM and uh this is meant to
[13:24] remember the most recent episodes for
[13:27] immediate retrieval uh for the more
[13:29] recent events. And then we have a
[13:31] long-term memory that is uh uh
[13:33] persistent uh and that captures all the
[13:36] past episodes and we would uh uh this so
[13:41] this is a living machine. able to
[13:42] dynamically
[13:44] organize them and reorganize them and
[13:47] consolidate them over time and uh so
[13:50] that they are uh easier they they are
[13:55] easy to retrieval or to retrieve when we
[13:58] need to find the most relevant entities
[14:01] to any queries. And then we have the
[14:04] second piece is the profile memory and
[14:07] uh we actually are shipping a version.2
[14:10] next week and uh and it's going to
[14:13] generalize profile memory into semantic
[14:16] memory. U so profile memory is a special
[14:18] kind of semantic memory where it extract
[14:22] uh good attributes from all the
[14:25] episodes. So it's kind of a
[14:26] summarization or uh a projection from
[14:31] this episodes to particular dimensions
[14:33] and particular attributes and in
[14:35] particular profile memory is to enable
[14:39] the personalization of AI services. You
[14:42] can use profile profile memory to build
[14:45] uh portraits of any of the users whether
[14:48] it is internal employees or whether it's
[14:51] external clients. So that allow you to
[14:55] be more personalized uh in the service
[14:58] that agents can deliver uh to internal
[15:02] and external users. Uh and the third
[15:05] component is procedural memory and this
[15:08] is a memory of the procedures or of the
[15:11] workflows. Uh and that's is a sequence
[15:14] of actions uh that uh captures how
[15:18] certain task to be done. you know early
[15:21] primitive form of procedures is like
[15:24] closed skills if you're familiar with
[15:26] that that was announced maybe 10 months
[15:27] ago uh and uh and that's kind of the
[15:31] primitive form and now they're more
[15:32] advanced form and there needs to be
[15:35] better management of all these uh
[15:37] procedures and I think this can directly
[15:39] contribute to the productivity boost
[15:42] within the enterprise and procedural
[15:44] memory is something we are still working
[15:46] on and that's not available in the open
[15:48] source project yet but The other two
[15:50] pieces are there. We are only at the
[15:53] beginning of journey. It's like 0.1 and
[15:55] we are continue to improve it. You'll
[15:56] see a big boost next week with our 0.2.
[16:00] Um we also included a couple agents
[16:03] within our memory and those are the
[16:05] orange boxes on the top. The data
[16:08] injection agent is responsible for
[16:10] connecting to all the data sources and
[16:13] to ingest them into various memories
[16:15] that we have. uh so that uh the uh
[16:19] memory can be enriched not only from all
[16:22] the chat history or the user agent or
[16:25] agent user model interactions uh but
[16:28] also potentially from user user
[16:30] interactions. So for example, we have uh
[16:34] built connectors to slack uh to emails
[16:38] to meeting transcripts so that you can
[16:42] capture all those episodes into the
[16:45] memory system as well. The second agent
[16:48] is contact retrieval agent. And the
[16:51] contact retrieval agent is responsible
[16:54] for when you see a query that you can
[16:56] translate that query into subqueries
[16:59] into different pieces of memory or
[17:02] potentially into other uh context
[17:05] sources like rack systems where many
[17:08] enterprise have them today. And it is
[17:10] then responsible for assemble them
[17:13] together to form the overall uh context
[17:17] enrichment uh to queries. And so this
[17:20] can be done by just adding with it
[17:22] within the contract window or to be
[17:25] queried by models directly through a
[17:27] reasoning step through the MCP server
[17:31] and and we have a very early primitive
[17:34] forms of these two agents today and we
[17:37] are actively working to uh advance them
[17:40] and we believe these will be key models
[17:42] to enable an overall context engineering
[17:46] system. Um, so that's a uh uh overview
[17:50] of the Yes, please.
[18:02] >> Yes. So this this whole layer is
[18:04] software only and uh it can be deployed
[18:09] anywhere. It can be deployed uh in the
[18:11] cloud. AWS our co-host there can be
[18:15] deployed on the on the cloud. It can be
[18:17] deployed in your own data center. It can
[18:20] be deployed on your own laptop. It can
[18:23] potentially deploy in your phone as
[18:25] well. So so this is a software layer
[18:27] that can be deployed anywhere. And I
[18:28] didn't draw here. We do use some
[18:30] databases underneath as well that help
[18:33] us as sort of really the storage
[18:36] uh for these memory entities and for
[18:39] these profiles. And you can use this and
[18:41] then it should be
[18:43] >> Yeah. And the working memory uh runs on
[18:47] top of VRAM and persistent memory runs
[18:50] really on top of persistent storage. Uh
[18:53] and it can be any any of them. Yes.
[18:56] >> So this language agnostic. So for
[18:58] example the majority of my customers
[19:01] data are English. Would that work?
[19:04] >> Yeah. Yeah. It's a language agnostic. So
[19:07] it can be any of the languages.
[19:13] We first asked question thinking this
[19:15] programming language but programming
[19:17] language also you know the Python SDK
[19:20] will be only for Python but the other uh
[19:22] APIs because Python is the most popular
[19:25] but it can support the other languages
[19:27] other programming language as well. But
[19:28] in terms of data today it does support
[19:31] any kind of text data whatever is the
[19:34] language we do not support you know
[19:37] multimodel data yet uh but those will be
[19:40] coming also and those will be important
[19:42] such as voice or or or pictures or
[19:46] videos and then I think those will be
[19:48] important part of the memory also but we
[19:50] are only at the beginning of this
[19:51] journey here um and and I we are
[19:55] certainly not the only one doing it. I
[19:57] think there are many uh comrades or
[20:00] maybe there are better words for that
[20:03] people who share the same vision here
[20:05] and who are who are building it and uh
[20:08] and and these are uh some of the results
[20:11] we we gathered from the published
[20:14] materials
[20:15] uh of the benchmark uh using a benchmark
[20:19] for local and and that seems to be a
[20:22] popular benchmark with all the people
[20:25] who are working on memory.
[20:26] Um and uh uh actually I forgot to change
[20:30] the typo because we our latest
[20:34] 0.1
[20:36] is achieving the statewide uh in terms
[20:38] of local performance. Uh as you see some
[20:41] of the other players they will probably
[20:44] publish new data to elevate continue to
[20:47] elevate their performance as well. But
[20:49] from what we gathered from the published
[20:52] material, we saw that open AI uh is
[20:56] about 52%. So how this benchmark works
[20:59] is it feeds 10 long conversations
[21:03] u into the memory system and then it
[21:06] generate about 1900 questions. It would
[21:09] ask the model which is integrated with
[21:11] that particular memory system. If it
[21:14] doesn't have a memory, it it only will
[21:16] get very small percentage of those
[21:18] questions correctly. But with a memory
[21:20] system now, I can't answer all of them
[21:22] correctly. So the openi answer about 52%
[21:25] correctly and man with a memory system
[21:28] will event
[21:37] the numbers they have is what I got is
[21:39] 66 to 75%. they probably have new or
[21:43] better numbers uh as we make progress.
[21:46] Um I think uh uh when we launched the
[21:50] project in September we published
[21:52] numbers at 84.87%
[21:55] and in the last couple months uh with
[21:57] our new improvement uh our latest
[22:00] performance 93.25%.
[22:03] And from the numbers that we have seen I
[22:05] think this is a state of art uh in the
[22:07] episodic memory performance. uh we do
[22:10] see a new announcement from uh uh last
[22:13] weekend uh from uh Evermind and who I
[22:17] think will tank will be presenting
[22:19] today. They are publishing 92.3%
[22:23] which is a very good result uh as well.
[22:25] So I think now this is just one of the
[22:28] benchmarks. There are there are other
[22:29] benchmarks
[22:30] that are uh existing and I believe
[22:34] there's room for more benchmarks to be
[22:36] created to explore uh a deeper aspect of
[22:41] memory and uh and that's an area I think
[22:44] the industry can come together to
[22:45] advance so that we have a common
[22:47] framework. How do we make progress on
[22:51] the actual memory system that we build?
[22:55] Okay. Um, now I will end my presentation
[22:59] with a demo. Uh so this is a demo of of
[23:04] a map machine uh system at work in a
[23:07] typical enterprise environment which is
[23:10] our focus and our objective is to uh
[23:15] build this uh system for the enterprises
[23:18] that within their control whe either
[23:21] deployed in their own version private
[23:23] cloud AWS or any other cloud systems and
[23:27] uh or in into their own data center and
[23:30] uh there'll be users using it and in
[23:32] this demo it'll be Tom I'll be the
[23:34] enterprise user here and we have
[23:37] integrated with various data sources
[23:39] like emails and flags and the meeting
[23:41] transcripts and we also have integrated
[23:44] with two of the most common super agents
[23:48] uh one is maybe the most common super
[23:50] agent that's the uh the chatbot that
[23:53] open
[23:55] that we have integrated with that and we
[23:57] have also integrated with closed desktop
[24:00] That's uh actually the gaming uh perhaps
[24:04] is even more popular than CHP within the
[24:08] enterprise environment. It is part
[24:11] particularly good for coding as well as
[24:14] some other professional tasks such as
[24:16] creating powerpoints etc. So uh we we
[24:20] actually think it's a very common
[24:22] scenario for a typical uh worker like
[24:26] Charles uh to use more than one models
[24:30] and more than one agents for my daily
[24:33] work. Uh and in this case so here is a
[24:36] demo scenario I'm going to a typical
[24:40] knowledge worker in enterprise. I like
[24:42] to use checkp for general tasks which is
[24:45] quite common but I do like flow for
[24:48] specific professional task
[24:51] poweroint and other things. Uh now the
[24:53] task I got is I needed needed to prepare
[24:55] a presentation next week with a customer
[24:59] apex it's a it's a makeup name uh the uh
[25:02] with a with a particular customer and
[25:05] how I go about and doing it is I first
[25:07] work with KBD
[25:09] um and KPD is powered by mechain behind
[25:13] and uh so it will be able to uh retrieve
[25:17] the relevant information from my memory
[25:20] system about uh Apex tack from all the
[25:24] meeting minutes and emails and Slack
[25:27] conversation that we've had about this
[25:30] uh uh customer as well as from past AI
[25:34] uh chat box or chat history about them
[25:37] and it'll be able to summarize them
[25:39] deliver me information about the
[25:41] customer uh it will remember things
[25:44] about the product I'm going to present
[25:46] and it's also going to remember about me
[25:48] so it's gonna the profile memory going
[25:50] to deliver uh what is Charles like? uh
[25:54] what uh do I like you know what type of
[25:58] slides I like to make and what type of
[26:00] flow I like to follow and all those
[26:02] information will be automatically served
[26:05] as context into models and model will be
[26:08] able to generate in this case open AI
[26:10] model will be generating a presentation
[26:12] outline and but I don't like for the
[26:16] actual poweroint it's actually not
[26:19] yeah it's not good at that so then I
[26:22] will will you know bring up my clo uh
[26:25] desktop and uh now clo all those
[26:28] conversations with JBT is automatically
[26:30] real time captured by membership memory
[26:32] system and and clo will be able to use
[26:35] that interaction give me some additional
[26:37] advices and actually go ahead and
[26:39] generate a particle. So this shows a
[26:41] typical scenario where a knowledge
[26:44] worker like myself can use multiple
[26:47] agents, multiple models or connected
[26:49] through a memory fabric to getting a
[26:51] task done very easily.
[26:53] >> Yes.
[26:54] >> So when you use the chat GBT right based
[26:58] on the me machine do your interactions
[27:01] feed back to the memory?
[27:03] >> Yes.
[27:04] >> Yeah. when we are actually using the CHP
[27:06] those interactions are automatically in
[27:09] the background uh being remembered by
[27:12] yes and and and now let me show the
[27:15] video here
[27:17] all right so
[27:20] let's see I think it should be
[27:24] sorry the text is pretty small but I
[27:25] basically ask you to uh you're creating
[27:28] a presentation and if you recall what
[27:30] you can remember about the customer okay
[27:33] it it goes out Here they will remember
[27:35] about customers the key people the pain
[27:37] point they have etc etc and then it
[27:40] remembers about the product you know the
[27:42] product I'm presenting what they recall
[27:44] from the memory and then it records
[27:47] about me you know my styles and so on
[27:50] but it does ask me a few more questions
[27:51] uh in order for it to creating this
[27:53] presentation and uh and so now I'll give
[27:57] the answers to these questions uh the
[28:00] meeting purpose design tone and comedy
[28:02] slides etc.
[28:04] Okay. Now the open AI models with all
[28:07] these information uh can generate a
[28:10] slide uh outline here the slide one kind
[28:14] of the context challenge inside the
[28:15] solution etc. It create a slide
[28:19] presentation. Uh and uh yeah, business
[28:22] impact why now next steps. Okay, that's
[28:24] great. Uh I like it. Uh but I don't
[28:28] really like JPT to do my actual slide
[28:31] work. So I go to clone uh desktop and
[28:35] this is a question the clone. So I say
[28:37] do you remember what I just created? It
[28:40] says okay I I do see it and uh let me
[28:43] retrieve it from memory. The clone is
[28:44] also powered by me. the same memory
[28:47] machine instance that's connected to
[28:48] KPBD and it remembers those eight slides
[28:52] in slightly different formats but
[28:54] capture the key information about those
[28:56] uh eight eight slides. uh here are the
[29:00] you know other information remember from
[29:02] the memory the key context there and
[29:05] then uh then I asked you know I asked
[29:08] for second opinion here now and I think
[29:10] many of us too sometimes we ask multiple
[29:12] models to second opinion so I said okay
[29:16] would you have any suggestion any
[29:17] improvement you can make to this outline
[29:20] says okay I have five five suggestions
[29:23] uh something is missing something is too
[29:26] abstract good and uh and here are some
[29:30] suggestion I have. Now it's up to me to
[29:32] decide what are those suggestion I take,
[29:34] which one I don't take. And in this
[29:36] example, I say great um but just go with
[29:40] number one like the first first
[29:41] suggestion and uh ignore the rest and go
[29:44] ahead and create the PowerPoint. Uh as
[29:47] of now 12 goes to work um and uh it will
[29:51] take a second. In fact, it took about uh
[29:55] you know 10 minutes or so for it to work
[29:57] work through it. Then we fast forward a
[29:59] little bit uh and to to actually uh
[30:02] create the the slides here. So so you'll
[30:05] see it's creating
[30:07] actually nine nine slides here uh as we
[30:10] have taken one suggestion from it and
[30:13] voila you have the uh the slides is
[30:16] created. uh and this is a seamless
[30:19] process where you use multiple agents
[30:21] powered by different models but
[30:22] connected to a common converg management
[30:25] system that can deliver uh the output
[30:28] you need and uh and this is a simple
[30:30] example uh pretty common scenario I
[30:33] think all of us do that there are many
[30:35] other common scenarios where a common
[30:38] converge AI memory fabric can enable
[30:42] this multi- aent collaboration to happen
[30:45] seamlessly
[30:46] Okay. All right. So that's the uh uh
[30:49] example I have and then as you see there
[30:52] there's many use cases for something
[30:54] like this. Now whether you're creating
[30:57] assistance to various roles, whether
[31:00] it's executives, creative content
[31:02] creators, or whether you're serving
[31:04] external clients uh or financial advice
[31:07] or customer support, a memory system is
[31:12] quintessential uh in delivering uh the
[31:16] state-of-the-art agents uh for these
[31:18] purposes. Uh so try it out. Uh here is a
[31:23] barcode for the project and this will
[31:26] connect you to GitHub to our playground
[31:30] to uh uh other resources, documentations
[31:33] and so on about the project that will
[31:35] dive into much more details and my
[31:37] presentation and we also have a team
[31:40] here uh that you can scan this to
[31:43] schedule a meeting that can happen
[31:44] today. We have a room oneonone with our
[31:47] team to learn more about the memory. And
[31:50] for those folks on the zoom, you know,
[31:52] you can also use this to to schedule
[31:55] meetings uh with us uh to dive into more
[31:58] details. So, thank you very much.